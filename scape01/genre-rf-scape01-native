#!/bin/bash

export HADOOP_HOME=/usr/lib/hadoop
export HADOOP_CONF_DIR=/etc/hadoop/conf
export YARN_CONF_DIR=$HADOOP_CONF_DIR

_now=$(date +"%Y-%m-%d_%H-%M-%S")
_output_stderr_dir="output-native-stderr"
_output_stdout_dir="output-native-stdout"
_jar_dir="jar"
_conf_dir="conf-single"

_cores_max="2"
_exec_mem="4G"

(/opt/spark-2.1/bin/spark-submit \
  --jars $_jar_dir/config-1.3.1.jar,$_jar_dir/scallop_2.11-2.1.1.jar,$_jar_dir/spark-measure_2.11-0.1-SNAPSHOT.jar\
  --properties-file spark-submit-scape01-native.conf\
  --conf "spark.cores.max=$_cores_max"\
  --conf "spark.executor.memory=$_exec_mem"\
  --class "GenreRandomForestApp" $_jar_dir/machine-learning_2.11-1.0.jar\
  --config $_conf_dir/genre-randomForest.conf | \
    tee $_output_stdout_dir/$_now-$_filename-cores-$_cores_max-mem-$_exec_mem-stdout.log) 3>&1 1>&2 2>&3 | \
	tee $_output_stderr_dir/$_now-$_filename-cores-$_cores_max-mem-$_exec_mem-stderr.log 
    
#  --deploy-mode client\
#  --conf "spark.executors.instances=$_exec_inst"\
#  --conf "spark.executor.cores=$_exec_cores"\
#  --conf "spark.executor.memory=$_exec_mem"\
